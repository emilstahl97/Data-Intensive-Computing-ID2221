{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd17309b",
   "metadata": {},
   "source": [
    "# ID2221 HT21 Lab2\n",
    "\n",
    "## Agenda:\n",
    "\n",
    "We will go through the following:\n",
    "- ''Lab2_01_Structured_Data_Processing.ipynb''\n",
    "- ''Lab2_02_SparkSQL_DataFrame.ipynb''\n",
    "- ''Lab2_03_SparkSQL_Dataset.html''\n",
    "\n",
    "In the last lab session, we looked at how to set up Spark environment in a native environment (Your local machine or a virual machine).\n",
    "\n",
    "In this session, we will look at some alternatives for using Spark.\n",
    "- Running a Docker container to use jupyter notebook/lab to access spark.\n",
    "- Using Databricks Community Edition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89727e88",
   "metadata": {},
   "source": [
    "# Structured Data Processing\n",
    "- Spark has two notions of structured collections:\n",
    "  - DataFrames\n",
    "  - Datasets\n",
    "- They are distributed table-like collections with well-defined rows and columns.\n",
    "- They represent immutable lazily evaluated plans.\n",
    "- When an action is performed on them, Spark performs the actual transformations\n",
    "and return the result.\n",
    "\n",
    "Code examples used in this notebook can be found [here](https://id2221kth.github.io/slides/2021/07_structured_data_processing.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cd9db18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://e507a37883e8:4041\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1632307403324)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook is tested with Spark 3.1.2.\n",
      "You are using 3.1.2.\n"
     ]
    }
   ],
   "source": [
    "println(s\"This notebook is tested with Spark 3.1.2.\\nYou are using ${sc.version}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1cb7c",
   "metadata": {},
   "source": [
    "## Read data using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779cd9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "// people.json\n",
    "\n",
    "// {\"name\":\"Michael\", \"age\":15, \"id\":12}\n",
    "// {\"name\":\"Andy\", \"age\":30, \"id\":15}\n",
    "// {\"name\":\"Justin\", \"age\":19, \"id\":20}\n",
    "// {\"name\":\"Andy\", \"age\":12, \"id\":15}\n",
    "// {\"name\":\"Jim\", \"age\":19, \"id\":20}\n",
    "// {\"name\":\"Andy\", \"age\":12, \"id\":10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ae3880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people_df: org.apache.spark.sql.DataFrame = [age: bigint, id: bigint ... 1 more field]\n",
       "res2: org.apache.spark.sql.types.StructType = StructType(StructField(age,LongType,true), StructField(id,LongType,true), StructField(name,StringType,true))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Data sources supported by Spark.\n",
    "// • CSV, JSON, Parquet, ORC, JDBC/ODBC connections, Plain-text files\n",
    "// • Cassandra, HBase, MongoDB, AWS Redshift, XML, etc.\n",
    "\n",
    "// For example,\n",
    "val people_df = spark.read.format(\"json\").load(\"people.json\")\n",
    "people_df.schema\n",
    "\n",
    "\n",
    "// val peopleCsv = spark.read.format(\"csv\")\n",
    "//                         .option(\"sep\", \";\")\n",
    "//                         .option(\"inferSchema\", \"true\")\n",
    "//                         .option(\"header\", \"true\")\n",
    "//                         .load(\"people.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d72bee",
   "metadata": {},
   "source": [
    "## Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e14de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.sql.Column = age\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//  col returns a reference to a column\n",
    "col(\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6ebf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.sql.Column = EXP(age + 5 < 32)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// expr performs transformations on a column\n",
    "exp(\"age + 5 < 32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8bec3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[String] = Array(age, id, name)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//  columns returns all columns on a DataFrame\n",
    "people_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddbf322d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.Column = name\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Different ways to refer to a column\n",
    "people_df.col(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb6c61f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.sql.Column = name\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col(\"name\")\n",
    "column(\"name\")\n",
    "'name\n",
    "$\"name\"\n",
    "expr(\"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1d414",
   "metadata": {},
   "source": [
    "## Row\n",
    "- A row is a record of data.\n",
    "- They are of type ``Row``.\n",
    "- Rows do not have schemas.\n",
    "  -  The order of values should be the same order as the schema of the DataFrame to which they might be appended.\n",
    "- To access data in rows, you need to specify the position that you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2047b2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "my_row: org.apache.spark.sql.Row = [Seif,65,0]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val my_row = Row(\"Seif\", 65, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3257b483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Any = Seif\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_row(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f3f61ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: String = Seif\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_row(0).asInstanceOf[String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85444303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: String = Seif\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_row.getString(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4920e144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Int = 65\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_row.getInt(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ba489",
   "metadata": {},
   "source": [
    "## Creating a DataFrame\n",
    "There are two ways to create a DataFrame:\n",
    "1. From an RDD.\n",
    "2. From raw data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a50a5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple_rdd: org.apache.spark.rdd.RDD[(String, Int, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:26\n",
       "tuple_df: org.apache.spark.sql.DataFrame = [name: string, age: int ... 1 more field]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tuple_rdd = sc.parallelize(Array((\"seif\", 65, 0), (\"amir\", 40, 1)))\n",
    "val tuple_df = tuple_rdd.toDF(\"name\", \"age\", \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87b70188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(name: String, age: Int, id: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca55d29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_rdd: org.apache.spark.rdd.RDD[Person] = ParallelCollectionRDD[5] at parallelize at <console>:28\n",
       "my_df: org.apache.spark.sql.DataFrame = [name: string, age: int ... 1 more field]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val my_rdd = sc.parallelize(Array(Person(\"seif\", 65, 0), Person(\"amir\", 40, 1)))\n",
    "val my_df = my_rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e464384",
   "metadata": {},
   "source": [
    "## DataFrame Transformations\n",
    "- Add and remove rows or columns\n",
    "- Transform a row into a column (or vice versa)\n",
    "- Change the order of rows based on the values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b00449fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+\n",
      "|   name|age| id|\n",
      "+-------+---+---+\n",
      "|Michael| 15| 12|\n",
      "|   Andy| 30| 15|\n",
      "+-------+---+---+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+---------+\n",
      "|   name|(age + 3)|\n",
      "+-------+---------+\n",
      "|Michael|       18|\n",
      "|   Andy|       33|\n",
      "| Justin|       22|\n",
      "|   Andy|       15|\n",
      "|    Jim|       22|\n",
      "|   Andy|       15|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// select and selectExpr allow to do the DataFrame equivalent of SQL queries on a table of data.\n",
    "// select\n",
    "people_df.select(\"name\", \"age\", \"id\").show(2)\n",
    "people_df.select(col(\"name\"), expr(\"age + 3\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aef538bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+\n",
      "|   name|   name|   name|   name|   name|\n",
      "+-------+-------+-------+-------+-------+\n",
      "|Michael|Michael|Michael|Michael|Michael|\n",
      "|   Andy|   Andy|   Andy|   Andy|   Andy|\n",
      "| Justin| Justin| Justin| Justin| Justin|\n",
      "|   Andy|   Andy|   Andy|   Andy|   Andy|\n",
      "|    Jim|    Jim|    Jim|    Jim|    Jim|\n",
      "|   Andy|   Andy|   Andy|   Andy|   Andy|\n",
      "+-------+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.select(col(\"name\"), column(\"name\"), 'name, $\"name\", expr(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "579042d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+--------+---+\n",
      "|age| id|   name|teenager| ID|\n",
      "+---+---+-------+--------+---+\n",
      "| 15| 12|Michael|    true| 12|\n",
      "| 30| 15|   Andy|   false| 15|\n",
      "| 19| 20| Justin|    true| 20|\n",
      "| 12| 15|   Andy|    true| 15|\n",
      "| 19| 20|    Jim|    true| 20|\n",
      "| 12| 10|   Andy|    true| 10|\n",
      "+---+---+-------+--------+---+\n",
      "\n",
      "+------------------+--------------------+-------+\n",
      "|          avg(age)|count(DISTINCT name)|sum(id)|\n",
      "+------------------+--------------------+-------+\n",
      "|17.833333333333332|                   4|     92|\n",
      "+------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// selectExpr\n",
    "// Spark SQL function selectExpr() is similar to select(), \n",
    "// the difference being it takes a set of SQL expressions in a string to execute. \n",
    "// This gives an ability to run SQL like expressions without creating a temporary table and views.\n",
    "\n",
    "// Star Syntax basically selects all the columns similar to select * in sql\n",
    "people_df.selectExpr(\"*\", \"(age < 20) as teenager\", \"id as ID\").show()\n",
    "people_df.selectExpr(\"avg(age)\", \"count(distinct(name))\", \"sum(id)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aaa7b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 15| 12|Michael|\n",
      "| 19| 20| Justin|\n",
      "| 12| 15|   Andy|\n",
      "| 19| 20|    Jim|\n",
      "| 12| 10|   Andy|\n",
      "+---+---+-------+\n",
      "\n",
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 15| 12|Michael|\n",
      "| 19| 20| Justin|\n",
      "| 12| 15|   Andy|\n",
      "| 19| 20|    Jim|\n",
      "| 12| 10|   Andy|\n",
      "+---+---+-------+\n",
      "\n",
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 15| 12|Michael|\n",
      "| 19| 20| Justin|\n",
      "| 12| 15|   Andy|\n",
      "| 19| 20|    Jim|\n",
      "| 12| 10|   Andy|\n",
      "+---+---+-------+\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|    Jim|\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// filter and where both filter rows.\n",
    "\n",
    "people_df.filter(col(\"age\") < 20).show()\n",
    "people_df.filter(\"age < 20\").show()\n",
    "\n",
    "people_df.where(\"age < 20\").show()\n",
    "\n",
    "// distinct can be used to extract unique rows.\n",
    "people_df.select(\"name\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c33e9604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+--------+\n",
      "|age| id|   name|teenager|\n",
      "+---+---+-------+--------+\n",
      "| 15| 12|Michael|    true|\n",
      "| 30| 15|   Andy|   false|\n",
      "| 19| 20| Justin|    true|\n",
      "| 12| 15|   Andy|    true|\n",
      "| 19| 20|    Jim|    true|\n",
      "| 12| 10|   Andy|    true|\n",
      "+---+---+-------+--------+\n",
      "\n",
      "+---+---+-------+--------+\n",
      "|age| id|   name|teenager|\n",
      "+---+---+-------+--------+\n",
      "| 15| 12|Michael|    true|\n",
      "| 30| 15|   Andy|   false|\n",
      "| 19| 20| Justin|    true|\n",
      "| 12| 15|   Andy|    true|\n",
      "| 19| 20|    Jim|    true|\n",
      "| 12| 10|   Andy|    true|\n",
      "+---+---+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// withColumn adds a new column to a DataFrame\n",
    "people_df.withColumn(\"teenager\", expr(\"age < 20\")).show()\n",
    "\n",
    "people_df.selectExpr(\"*\", \"(age < 20) as teenager\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "167ac919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp_df: org.apache.spark.sql.DataFrame = [age: bigint, id: bigint]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// withColumnRenamed renames a column\n",
    "people_df.withColumnRenamed(\"name\", \"username\").columns\n",
    "\n",
    "// drop removes a column\n",
    "val temp_df = people_df.drop(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a02041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|age| id|\n",
      "+---+---+\n",
      "| 15| 12|\n",
      "| 30| 15|\n",
      "| 19| 20|\n",
      "| 12| 15|\n",
      "| 19| 20|\n",
      "| 12| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6746806b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 15| 12|Michael|\n",
      "| 30| 15|   Andy|\n",
      "| 19| 20| Justin|\n",
      "| 12| 15|   Andy|\n",
      "| 19| 20|    Jim|\n",
      "| 12| 10|   Andy|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b4dcf",
   "metadata": {},
   "source": [
    "## DataFrame Actions\n",
    "Like RDDs, DataFrames also have their own set of actions.\n",
    "- ``collect``: returns an array that contains all of rows in this DataFrame.\n",
    "- ``count``: returns the number of rows in this DataFrame.\n",
    "- ``first`` and ``head``: returns the first row of the DataFrame.\n",
    "- ``show``: displays the top 20 rows of the DataFrame in a tabular form.\n",
    "- ``take``: returns the first n rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "663703a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 15| 12|Michael|\n",
      "| 30| 15|   Andy|\n",
      "| 19| 20| Justin|\n",
      "| 12| 15|   Andy|\n",
      "| 19| 20|    Jim|\n",
      "| 12| 10|   Andy|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b6ed9f",
   "metadata": {},
   "source": [
    "### Summarizing a Complete DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1a69527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|count(age)|\n",
      "+----------+\n",
      "|         6|\n",
      "+----------+\n",
      "\n",
      "+--------------------+\n",
      "|count(DISTINCT name)|\n",
      "+--------------------+\n",
      "|                   4|\n",
      "+--------------------+\n",
      "\n",
      "+-----------+---------+\n",
      "|first(name)|last(age)|\n",
      "+-----------+---------+\n",
      "|    Michael|       12|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// count returns the total number of values\n",
    "people_df.select(count(\"age\")).show()\n",
    "\n",
    "// countDistinct returns the number of unique groups\n",
    "people_df.select(countDistinct(\"name\")).show()\n",
    "\n",
    "// first and last return the first and last value of a DataFrame\n",
    "people_df.select(first(\"name\"), last(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1f5f9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+\n",
      "|min(name)|max(age)|max(id)|\n",
      "+---------+--------+-------+\n",
      "|     Andy|      30|     20|\n",
      "+---------+--------+-------+\n",
      "\n",
      "+--------+\n",
      "|sum(age)|\n",
      "+--------+\n",
      "|     107|\n",
      "+--------+\n",
      "\n",
      "+------------------+\n",
      "|          avg(age)|\n",
      "+------------------+\n",
      "|17.833333333333332|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// min and max extract the minimum and maximum values from a DataFrame\n",
    "people_df.select(min(\"name\"), max(\"age\"), max(\"id\")).show()\n",
    "\n",
    "// sum adds all the values in a column\n",
    "people_df.select(sum(\"age\")).show()\n",
    "\n",
    "// avg calculates the average\n",
    "people_df.select(avg(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b2a60",
   "metadata": {},
   "source": [
    "### Group By\n",
    "- Perform aggregations on groups in the data.\n",
    "- Typically on categorical data.\n",
    "- We do this grouping in two phases:\n",
    "  1. Specify the column(s) on which we would like to group.\n",
    "  2. Specify the aggregation(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfb9eff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 15| 12|Michael|\n",
      "| 30| 15|   Andy|\n",
      "| 19| 20| Justin|\n",
      "| 12| 15|   Andy|\n",
      "| 19| 20|    Jim|\n",
      "| 12| 10|   Andy|\n",
      "+---+---+-------+\n",
      "\n",
      "+-------+------+\n",
      "|   name|ageagg|\n",
      "+-------+------+\n",
      "|    Jim|     1|\n",
      "|Michael|     1|\n",
      "|   Andy|     3|\n",
      "| Justin|     1|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()\n",
    "\n",
    "// Grouping with expressions\n",
    "\n",
    "// Rather than passing that function as an expression into a select statement, \n",
    "// we specify it as within agg.\n",
    "people_df.groupBy(\"name\").agg(count(\"age\").alias(\"ageagg\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a6ba075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+-------+\n",
      "|   name|count(age)|avg(age)|max(id)|\n",
      "+-------+----------+--------+-------+\n",
      "|    Jim|         1|    19.0|     20|\n",
      "|Michael|         1|    15.0|     12|\n",
      "|   Andy|         3|    18.0|     15|\n",
      "| Justin|         1|    19.0|     20|\n",
      "+-------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Grouping with Maps\n",
    "\n",
    "// Specify transformations as a series of Maps\n",
    "// The key is the column, and the value is the aggregation function (as a string).\n",
    "people_df.groupBy(\"name\").agg(\"age\" -> \"count\", \"age\" -> \"avg\", \"id\" -> \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1c9d0",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "- Computing some aggregation on a specific ``window`` of data.\n",
    "- The window determines which rows will be passed in to this function.\n",
    "- You define them by using a reference to the current data.\n",
    "- A group of rows is called a ``frame``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09a80862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------------------+\n",
      "|   name|age|           avg_age|\n",
      "+-------+---+------------------+\n",
      "|Michael| 15|              22.5|\n",
      "|   Andy| 30|21.333333333333332|\n",
      "| Justin| 19|20.333333333333332|\n",
      "|   Andy| 12|16.666666666666668|\n",
      "|    Jim| 19|14.333333333333334|\n",
      "|   Andy| 12|              15.5|\n",
      "+-------+---+------------------+\n",
      "\n",
      "22.5\n",
      "21.333333333333332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions.col\n",
       "windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@4c08012b\n",
       "avgAge: org.apache.spark.sql.Column = avg(age) OVER (ROWS BETWEEN -1 FOLLOWING AND 1 FOLLOWING)\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "// Unlike grouping, here each row can fall into one or more frames.\n",
    "val windowSpec = Window.rowsBetween(-1, 1)\n",
    "val avgAge = avg(col(\"age\")).over(windowSpec)\n",
    "\n",
    "people_df.select(col(\"name\"), col(\"age\"), avgAge.alias(\"avg_age\")).show()\n",
    "\n",
    "println((15.0+30.0)/2)\n",
    "\n",
    "println((15.0+30.0+19.0)/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb59b20",
   "metadata": {},
   "source": [
    "### Joins\n",
    "- Joins are relational constructs you use to combine relations together.\n",
    "- Combine rows from two or more tables, based on a related column between them.\n",
    "- Different join types ([spark_examples](https://www.educba.com/join-in-spark-sql/)): \n",
    "  - inner join,\n",
    "  - outer join,\n",
    "  - left outer join,\n",
    "  - right outer join,\n",
    "  - left semi join,\n",
    "  - left anti join,\n",
    "  - cross join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69eb58ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+\n",
      "| id|   name|group_id|\n",
      "+---+-------+--------+\n",
      "|  0|   Seif|       0|\n",
      "|  1|   Amir|       1|\n",
      "|  2|Sarunas|       1|\n",
      "+---+-------+--------+\n",
      "\n",
      "+---+----------+\n",
      "| id|department|\n",
      "+---+----------+\n",
      "|  0|  SICS/KTH|\n",
      "|  1|       KTH|\n",
      "|  2|      SICS|\n",
      "+---+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "person_df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]\n",
       "group_df: org.apache.spark.sql.DataFrame = [id: int, department: string]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val person_df = Seq((0, \"Seif\", 0), (1, \"Amir\", 1), (2, \"Sarunas\", 1)).toDF(\"id\", \"name\", \"group_id\")\n",
    "person_df.show()\n",
    "\n",
    "val group_df = Seq((0, \"SICS/KTH\"), (1, \"KTH\"), (2, \"SICS\")).toDF(\"id\", \"department\")\n",
    "group_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13bdc800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+---+----------+\n",
      "| id|   name|group_id| id|department|\n",
      "+---+-------+--------+---+----------+\n",
      "|  0|   Seif|       0|  0|  SICS/KTH|\n",
      "|  1|   Amir|       1|  1|       KTH|\n",
      "|  2|Sarunas|       1|  1|       KTH|\n",
      "+---+-------+--------+---+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "join_expression: org.apache.spark.sql.Column = (group_id = id)\n",
       "join_type: String = inner\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// inner join\n",
    "\n",
    "val join_expression = person_df.col(\"group_id\") === group_df.col(\"id\")\n",
    "val join_type = \"inner\"\n",
    "\n",
    "person_df.join(group_df, join_expression, join_type).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4218e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: org.apache.spark.sql.Column = (group_id = id)\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// For Spark, \"===\" is using the equalTo method\n",
    "// returns a column (which contains the result of the comparisons of the elements of two columns)\n",
    "person_df.col(\"group_id\") === group_df.col(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac09cafd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: Boolean = false\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// \"==\" checks if the two references point to the same object\n",
    "// returns a boolean\n",
    "person_df.col(\"group_id\") == group_df.col(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f2c3081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+---+----------+\n",
      "|  id|   name|group_id| id|department|\n",
      "+----+-------+--------+---+----------+\n",
      "|   1|   Amir|       1|  1|       KTH|\n",
      "|   2|Sarunas|       1|  1|       KTH|\n",
      "|null|   null|    null|  2|      SICS|\n",
      "|   0|   Seif|       0|  0|  SICS/KTH|\n",
      "+----+-------+--------+---+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "join_expression: org.apache.spark.sql.Column = (group_id = id)\n",
       "join_type: String = outer\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// outer join\n",
    "\n",
    "val join_expression = person_df.col(\"group_id\") === group_df.col(\"id\")\n",
    "val join_type = \"outer\"\n",
    "\n",
    "person_df.join(group_df, join_expression, join_type).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8346f",
   "metadata": {},
   "source": [
    "### Joins Communication Strategies\n",
    "- Shuffle join\n",
    "  - Every node talks to every other node\n",
    "  - Share data according to which node has a certain key or set of keys\n",
    "  - Join **big table** to **big table**\n",
    "- Broadcast join\n",
    "  - When the table is small enough to fit into the memory of a single worker node.\n",
    "  - Join **big table** to **small table**\n",
    "\n",
    "[Optional] If interested, check the following\n",
    "- [The art of joining in Spark](https://towardsdatascience.com/the-art-of-joining-in-spark-dcbd33d693c)\n",
    "- [Joining Strategies in Apache Spark](https://medium.com/nerd-for-tech/joining-strategies-in-apache-spark-f802a7dab150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb793f",
   "metadata": {},
   "source": [
    "## SQL\n",
    "\n",
    "You can run SQL queries on views/tables via the method sql on the SparkSession\n",
    "object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f454c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "// createOrReplaceTempView creates (or replaces) a lazily evaluated view\n",
    "people_df.createOrReplaceTempView(\"people_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a01cd595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 15| 12|Michael|\n",
      "| 30| 15|   Andy|\n",
      "| 19| 20| Justin|\n",
      "| 12| 15|   Andy|\n",
      "| 19| 20|    Jim|\n",
      "| 12| 10|   Andy|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from people_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "042efb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 15|\n",
      "| Justin| 19|\n",
      "|    Jim| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "teenagers_df: org.apache.spark.sql.DataFrame = [name: string, age: bigint]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val teenagers_df = spark.sql(\"SELECT name, age FROM people_view WHERE age BETWEEN 13 AND 19\")\n",
    "\n",
    "teenagers_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30c483",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c42dd4d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " cannot resolve '`id_num`' given input columns: [age, id, name]; line 1 pos 0;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: cannot resolve '`id_num`' given input columns: [age, id, name]; line 1 pos 0;",
      "'Filter ('id_num < 20)",
      "+- Relation[age#7L,id#8L,name#9] json",
      "",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:155)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:152)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:342)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:342)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:339)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:339)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:104)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:104)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:152)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:93)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:184)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:93)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:176)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:210)",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:216)",
      "  at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)",
      "  at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3720)",
      "  at org.apache.spark.sql.Dataset.filter(Dataset.scala:1596)",
      "  at org.apache.spark.sql.Dataset.filter(Dataset.scala:1610)",
      "  ... 36 elided",
      ""
     ]
    }
   ],
   "source": [
    "people_df.filter(\"id_num < 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60c37506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collected_people: Array[org.apache.spark.sql.Row] = Array([15,12,Michael], [30,15,Andy], [19,20,Justin], [12,15,Andy], [19,20,Jim], [12,10,Andy])\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val collected_people = people_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1f27672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res34: org.apache.spark.sql.types.StructType = StructType(StructField(age,LongType,true), StructField(id,LongType,true), StructField(name,StringType,true))\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d61cd96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Person(age: Long, id: Long, name: String)\n",
    "\n",
    "// To be able to work with the collected values, we should cast the Rows.\n",
    "// • How many columns?\n",
    "// • What types?\n",
    "\n",
    "val collected_list = collected_people.map{\n",
    "    row => (row(0).asInstanceOf[Long], row(1).asInstanceOf[Long], row(2).asInstanceOf[String])\n",
    "}\n",
    "\n",
    "// But, what if we cast the types wrong?\n",
    "// Wouldn’t it be nice if we could have both Spark SQL optimizations and typesafety?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733f5f2",
   "metadata": {},
   "source": [
    "### Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62047a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(name: String, age: BigInt, id: BigInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf64fa1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person_seq: Seq[Person] = List(Person(Max,33,0), Person(Adam,32,1))\n",
       "ds1: org.apache.spark.sql.Dataset[Person] = [name: string, age: decimal(38,0) ... 1 more field]\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val person_seq = Seq(Person(\"Max\", 33, 0), Person(\"Adam\", 32, 1))\n",
    "\n",
    "val ds1 = sc.parallelize(person_seq).toDS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eeecc744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds2: org.apache.spark.sql.Dataset[Person] = [age: bigint, id: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds2 = spark.read.format(\"json\").load(\"people.json\").as[Person]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00a610",
   "metadata": {},
   "source": [
    "### Dataset Transformations\n",
    "- Transformations on Datasets are the same as those that we had on DataFrames.\n",
    "- Datasets allow us to specify more complex and strongly typed transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d40c24a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(name: String, age: BigInt, id: BigInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "371b4341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people_ds: org.apache.spark.sql.Dataset[Person] = [age: bigint, id: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val people_ds = spark.read.format(\"json\").load(\"people.json\").as[Person].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9938d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 15| 12|Michael|\n",
      "| 30| 15|   Andy|\n",
      "| 19| 20| Justin|\n",
      "| 12| 15|   Andy|\n",
      "| 19| 20|    Jim|\n",
      "| 12| 10|   Andy|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25c2dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 15| 12|Michael|\n",
      "| 30| 15|   Andy|\n",
      "| 19| 20| Justin|\n",
      "| 12| 15|   Andy|\n",
      "| 19| 20|    Jim|\n",
      "| 12| 10|   Andy|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_ds.filter(people_ds(\"age\") < 40).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3904be16",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 87.0 failed 1 times, most recent failure: Lost task 0.0 in stage 87.0 (TID 1252) (e507a37883e8 executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @57af9fcd; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @7cd2d5b)",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 87.0 failed 1 times, most recent failure: Lost task 0.0 in stage 87.0 (TID 1252) (e507a37883e8 executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @57af9fcd; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @7cd2d5b)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:825)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:784)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:793)",
      "  ... 36 elided",
      "Caused by: java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @57af9fcd; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @7cd2d5b)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "people_ds.filter(x => x.age < 40).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36d881d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 88.0 failed 1 times, most recent failure: Lost task 0.0 in stage 88.0 (TID 1253) (e507a37883e8 executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @57af9fcd; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @7cd2d5b)",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 88.0 failed 1 times, most recent failure: Lost task 0.0 in stage 88.0 (TID 1253) (e507a37883e8 executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @57af9fcd; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @7cd2d5b)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:825)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:784)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:793)",
      "  ... 36 elided",
      "Caused by: java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @57af9fcd; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @7cd2d5b)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "people_ds.map(x => (x.name, x.age + 5, x.id)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b89194",
   "metadata": {},
   "source": [
    "# The END."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
