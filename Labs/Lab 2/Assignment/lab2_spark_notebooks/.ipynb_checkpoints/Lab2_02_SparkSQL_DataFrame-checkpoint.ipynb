{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark DataFrames - Scala\n",
    "\n",
    "[link_to_examples_used](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-scala.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://e507a37883e8:4040\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1632307229837)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook is tested with Spark 3.1.2.\n",
      "You are using 3.1.2.\n"
     ]
    }
   ],
   "source": [
    "println(s\"This notebook is tested with Spark 3.1.2.\\nYou are using ${sc.version}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Department\n",
       "defined class Employee\n",
       "defined class DepartmentWithEmployees\n",
       "department1: Department = Department(123456,Computer Science)\n",
       "department2: Department = Department(789012,Mechanical Engineering)\n",
       "department3: Department = Department(345678,Theater and Drama)\n",
       "department4: Department = Department(901234,Indoor Recreation)\n",
       "employee1: Employee = Employee(michael,armbrust,no-reply@berkeley.edu,100000)\n",
       "employee2: Employee = Employee(xiangrui,meng,no-reply@stanford.edu,120000)\n",
       "employee3: Employee = Employee(matei,null,no-reply@waterloo.edu,140000)\n",
       "employee4: Employee = Employee(null,wendell,no-reply@princeton.edu,160000)\n",
       "employee5: Employee = Employee(michael,jackson,no-reply@neverla.nd,80000)\n",
       "departmentWithEmployees1: DepartmentWithEmployees = DepartmentWithEmployees(Depar...\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create the case classes for our domain\n",
    "case class Department(id: String, name: String)\n",
    "case class Employee(firstName: String, lastName: String, email: String, salary: Int)\n",
    "case class DepartmentWithEmployees(department: Department, employees: Seq[Employee])\n",
    "\n",
    "// Create the Departments\n",
    "val department1 = new Department(\"123456\", \"Computer Science\")\n",
    "val department2 = new Department(\"789012\", \"Mechanical Engineering\")\n",
    "val department3 = new Department(\"345678\", \"Theater and Drama\")\n",
    "val department4 = new Department(\"901234\", \"Indoor Recreation\")\n",
    "\n",
    "// Create the Employees\n",
    "val employee1 = new Employee(\"michael\", \"armbrust\", \"no-reply@berkeley.edu\", 100000)\n",
    "val employee2 = new Employee(\"xiangrui\", \"meng\", \"no-reply@stanford.edu\", 120000)\n",
    "val employee3 = new Employee(\"matei\", null, \"no-reply@waterloo.edu\", 140000)\n",
    "val employee4 = new Employee(null, \"wendell\", \"no-reply@princeton.edu\", 160000)\n",
    "val employee5 = new Employee(\"michael\", \"jackson\", \"no-reply@neverla.nd\", 80000)\n",
    "\n",
    "// Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "val departmentWithEmployees1 = new DepartmentWithEmployees(department1, Seq(employee1, employee2))\n",
    "val departmentWithEmployees2 = new DepartmentWithEmployees(department2, Seq(employee3, employee4))\n",
    "val departmentWithEmployees3 = new DepartmentWithEmployees(department3, Seq(employee5, employee4))\n",
    "val departmentWithEmployees4 = new DepartmentWithEmployees(department4, Seq(employee2, employee3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrames from a list of the case classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "departmentsWithEmployeesSeq1: Seq[DepartmentWithEmployees] = List(DepartmentWithEmployees(Department(123456,Computer Science),List(Employee(michael,armbrust,no-reply@berkeley.edu,100000), Employee(xiangrui,meng,no-reply@stanford.edu,120000))), DepartmentWithEmployees(Department(789012,Mechanical Engineering),List(Employee(matei,null,no-reply@waterloo.edu,140000), Employee(null,wendell,no-reply@princeton.edu,160000))))\n",
       "df1: org.apache.spark.sql.DataFrame = [department: struct<id: string, name: string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:int>>]\n",
       "departmentsWithEmployeesSeq2: Seq[DepartmentWithEmployees] = List(DepartmentWithEmployees(Department(345678,Theater and Drama),List(Employee(michael,jackson,no-reply@neverla.nd,80000), Employee(null,wendell...\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val departmentsWithEmployeesSeq1 = Seq(departmentWithEmployees1, departmentWithEmployees2)\n",
    "val df1 = departmentsWithEmployeesSeq1.toDF()\n",
    "\n",
    "\n",
    "val departmentsWithEmployeesSeq2 = Seq(departmentWithEmployees3, departmentWithEmployees4)\n",
    "val df2 = departmentsWithEmployeesSeq2.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- employees: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- firstName: string (nullable = true)\n",
      " |    |    |-- lastName: string (nullable = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |    |    |-- salary: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- employees: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- firstName: string (nullable = true)\n",
      " |    |    |-- lastName: string (nullable = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |    |    |-- salary: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------------------------------------------------------------------------------------------------+\n",
      "|department                      |employees                                                                                            |\n",
      "+--------------------------------+-----------------------------------------------------------------------------------------------------+\n",
      "|{123456, Computer Science}      |[{michael, armbrust, no-reply@berkeley.edu, 100000}, {xiangrui, meng, no-reply@stanford.edu, 120000}]|\n",
      "|{789012, Mechanical Engineering}|[{matei, null, no-reply@waterloo.edu, 140000}, {null, wendell, no-reply@princeton.edu, 160000}]      |\n",
      "+--------------------------------+-----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------------------------------------------------------------------------------------+\n",
      "|department                 |employees                                                                                        |\n",
      "+---------------------------+-------------------------------------------------------------------------------------------------+\n",
      "|{345678, Theater and Drama}|[{michael, jackson, no-reply@neverla.nd, 80000}, {null, wendell, no-reply@princeton.edu, 160000}]|\n",
      "|{901234, Indoor Recreation}|[{xiangrui, meng, no-reply@stanford.edu, 120000}, {matei, null, no-reply@waterloo.edu, 140000}]  |\n",
      "+---------------------------+-------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unionDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [department: struct<id: string, name: string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:int>>]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unionDF = df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------------------------------------------------------------------------------------------------+\n",
      "|department                      |employees                                                                                            |\n",
      "+--------------------------------+-----------------------------------------------------------------------------------------------------+\n",
      "|{123456, Computer Science}      |[{michael, armbrust, no-reply@berkeley.edu, 100000}, {xiangrui, meng, no-reply@stanford.edu, 120000}]|\n",
      "|{789012, Mechanical Engineering}|[{matei, null, no-reply@waterloo.edu, 140000}, {null, wendell, no-reply@princeton.edu, 160000}]      |\n",
      "|{345678, Theater and Drama}     |[{michael, jackson, no-reply@neverla.nd, 80000}, {null, wendell, no-reply@princeton.edu, 160000}]    |\n",
      "|{901234, Indoor Recreation}     |[{xiangrui, meng, no-reply@stanford.edu, 120000}, {matei, null, no-reply@waterloo.edu, 140000}]      |\n",
      "+--------------------------------+-----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode the employees column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "explodeDF: org.apache.spark.sql.DataFrame = [col: struct<firstName: string, lastName: string ... 2 more fields>]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val explodeDF = unionDF.select(explode($\"employees\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|col                                               |\n",
      "+--------------------------------------------------+\n",
      "|{michael, armbrust, no-reply@berkeley.edu, 100000}|\n",
      "|{xiangrui, meng, no-reply@stanford.edu, 120000}   |\n",
      "|{matei, null, no-reply@waterloo.edu, 140000}      |\n",
      "|{null, wendell, no-reply@princeton.edu, 160000}   |\n",
      "|{michael, jackson, no-reply@neverla.nd, 80000}    |\n",
      "|{null, wendell, no-reply@princeton.edu, 160000}   |\n",
      "|{xiangrui, meng, no-reply@stanford.edu, 120000}   |\n",
      "|{matei, null, no-reply@waterloo.edu, 140000}      |\n",
      "+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodeDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten the fields of the employee class into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------------+------+\n",
      "|firstName|lastName|email                 |salary|\n",
      "+---------+--------+----------------------+------+\n",
      "|michael  |armbrust|no-reply@berkeley.edu |100000|\n",
      "|xiangrui |meng    |no-reply@stanford.edu |120000|\n",
      "|matei    |null    |no-reply@waterloo.edu |140000|\n",
      "|null     |wendell |no-reply@princeton.edu|160000|\n",
      "|michael  |jackson |no-reply@neverla.nd   |80000 |\n",
      "|null     |wendell |no-reply@princeton.edu|160000|\n",
      "|xiangrui |meng    |no-reply@stanford.edu |120000|\n",
      "|matei    |null    |no-reply@waterloo.edu |140000|\n",
      "+---------+--------+----------------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flattenDF: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flattenDF = explodeDF.select($\"col.*\")\n",
    "flattenDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use filter() to return the rows that match a predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [firstName: string, lastName: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filterDF = flattenDF.filter($\"firstName\" === \"xiangrui\" || $\"firstName\" === \"michael\").sort($\"lastName\".asc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------------------+------+\n",
      "|firstName|lastName|email                |salary|\n",
      "+---------+--------+---------------------+------+\n",
      "|michael  |armbrust|no-reply@berkeley.edu|100000|\n",
      "|michael  |jackson |no-reply@neverla.nd  |80000 |\n",
      "|xiangrui |meng    |no-reply@stanford.edu|120000|\n",
      "|xiangrui |meng    |no-reply@stanford.edu|120000|\n",
      "+---------+--------+---------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The where() clause is equivalent to filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "whereDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [firstName: string, lastName: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val whereDF = flattenDF.where($\"firstName\" === \"xiangrui\" || $\"firstName\" === \"michael\").sort($\"lastName\".asc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------------------+------+\n",
      "|firstName|lastName|email                |salary|\n",
      "+---------+--------+---------------------+------+\n",
      "|michael  |armbrust|no-reply@berkeley.edu|100000|\n",
      "|michael  |jackson |no-reply@neverla.nd  |80000 |\n",
      "|xiangrui |meng    |no-reply@stanford.edu|120000|\n",
      "|xiangrui |meng    |no-reply@stanford.edu|120000|\n",
      "+---------+--------+---------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whereDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace null values with -- using DataFrame Na function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------------+------+\n",
      "|firstName|lastName|email                 |salary|\n",
      "+---------+--------+----------------------+------+\n",
      "|michael  |armbrust|no-reply@berkeley.edu |100000|\n",
      "|xiangrui |meng    |no-reply@stanford.edu |120000|\n",
      "|matei    |null    |no-reply@waterloo.edu |140000|\n",
      "|null     |wendell |no-reply@princeton.edu|160000|\n",
      "|michael  |jackson |no-reply@neverla.nd   |80000 |\n",
      "|null     |wendell |no-reply@princeton.edu|160000|\n",
      "|xiangrui |meng    |no-reply@stanford.edu |120000|\n",
      "|matei    |null    |no-reply@waterloo.edu |140000|\n",
      "+---------+--------+----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattenDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nonNullDF: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nonNullDF = flattenDF.na.fill(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------------+------+\n",
      "|firstName|lastName|email                 |salary|\n",
      "+---------+--------+----------------------+------+\n",
      "|michael  |armbrust|no-reply@berkeley.edu |100000|\n",
      "|xiangrui |meng    |no-reply@stanford.edu |120000|\n",
      "|matei    |--      |no-reply@waterloo.edu |140000|\n",
      "|--       |wendell |no-reply@princeton.edu|160000|\n",
      "|michael  |jackson |no-reply@neverla.nd   |80000 |\n",
      "|--       |wendell |no-reply@princeton.edu|160000|\n",
      "|xiangrui |meng    |no-reply@stanford.edu |120000|\n",
      "|matei    |--      |no-reply@waterloo.edu |140000|\n",
      "+---------+--------+----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonNullDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve rows with missing firstName or lastName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterNonNullDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [firstName: string, lastName: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filterNonNullDF = nonNullDF.filter($\"firstName\" === \"--\" || $\"lastName\" === \"--\").sort($\"email\".asc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------------+------+\n",
      "|firstName|lastName|email                 |salary|\n",
      "+---------+--------+----------------------+------+\n",
      "|--       |wendell |no-reply@princeton.edu|160000|\n",
      "|--       |wendell |no-reply@princeton.edu|160000|\n",
      "|matei    |--      |no-reply@waterloo.edu |140000|\n",
      "|matei    |--      |no-reply@waterloo.edu |140000|\n",
      "+---------+--------+----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterNonNullDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example aggregations using agg() and countDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countDistinctDF: org.apache.spark.sql.DataFrame = [firstName: string, distinct_last_names: bigint]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Find the distinct last names for each first name\n",
    "val countDistinctDF = nonNullDF.select($\"firstName\", $\"lastName\")\n",
    "                                  .groupBy($\"firstName\")\n",
    "                                  .agg(countDistinct($\"lastName\") as \"distinct_last_names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|firstName|distinct_last_names|\n",
      "+---------+-------------------+\n",
      "|xiangrui |1                  |\n",
      "|matei    |1                  |\n",
      "|michael  |2                  |\n",
      "|--       |1                  |\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countDistinctDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the DataFrame and SQL query physical plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[firstName#136], functions=[count(distinct lastName#137)])\n",
      "+- Exchange hashpartitioning(firstName#136, 200), ENSURE_REQUIREMENTS, [id=#332]\n",
      "   +- *(2) HashAggregate(keys=[firstName#136], functions=[partial_count(distinct lastName#137)])\n",
      "      +- *(2) HashAggregate(keys=[firstName#136, lastName#137], functions=[])\n",
      "         +- Exchange hashpartitioning(firstName#136, lastName#137, 200), ENSURE_REQUIREMENTS, [id=#327]\n",
      "            +- *(1) HashAggregate(keys=[firstName#136, lastName#137], functions=[])\n",
      "               +- *(1) Project [coalesce(col#49.firstName, --) AS firstName#136, coalesce(col#49.lastName, --) AS lastName#137]\n",
      "                  +- Generate explode(employees#3), false, [col#49]\n",
      "                     +- Union\n",
      "                        :- LocalTableScan [employees#3]\n",
      "                        +- LocalTableScan [employees#12]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countDistinctDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[firstName#136], functions=[count(distinct lastName#137)])\n",
      "+- Exchange hashpartitioning(firstName#136, 200), ENSURE_REQUIREMENTS, [id=#386]\n",
      "   +- *(2) HashAggregate(keys=[firstName#136], functions=[partial_count(distinct lastName#137)])\n",
      "      +- *(2) HashAggregate(keys=[firstName#136, lastName#137], functions=[])\n",
      "         +- Exchange hashpartitioning(firstName#136, lastName#137, 200), ENSURE_REQUIREMENTS, [id=#381]\n",
      "            +- *(1) HashAggregate(keys=[firstName#136, lastName#137], functions=[])\n",
      "               +- *(1) Project [coalesce(col#49.firstName, --) AS firstName#136, coalesce(col#49.lastName, --) AS lastName#137]\n",
      "                  +- Generate explode(employees#3), false, [col#49]\n",
      "                     +- Union\n",
      "                        :- LocalTableScan [employees#3]\n",
      "                        +- LocalTableScan [employees#12]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// register the DataFrame as a temp view so that we can query it using SQL\n",
    "nonNullDF.createOrReplaceTempView(\"databricks_df_example\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  SELECT firstName, count(distinct lastName) as distinct_last_names\n",
    "  FROM databricks_df_example\n",
    "  GROUP BY firstName\n",
    "\"\"\").explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum up all the salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salarySumDF: org.apache.spark.sql.DataFrame = [sum(salary): bigint]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val salarySumDF = nonNullDF.agg(\"salary\" -> \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|1020000    |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salarySumDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the summary statistics for the salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            salary|\n",
      "+-------+------------------+\n",
      "|  count|                 8|\n",
      "|   mean|          127500.0|\n",
      "| stddev|28157.719063467175|\n",
      "|    min|             80000|\n",
      "|    max|            160000|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonNullDF.describe(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
